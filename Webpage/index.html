<!DOCTYPE html>
<html>

<head>
  <script src="https://code.iconify.design/1/1.0.7/iconify.min.js"></script>

    <title>EssAI</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
body {
  line-height: 1.5;
  margin: 0;
  font-size: 18px;
  font-family: Arial, Helvetica, sans-serif;
}

.header {
  background-color: #1abc9c;
  padding: 15px;
  text-align: center;
}

#navbar {
  overflow: hidden;
  background-color: #333;
  z-index: 999;
  /*position: absolute;*/
  /*padding-left: 250px;*/
  width: 1200px;
  margin: auto;
  /*padding: 50px;*/
}

#navbar a {
  float: left;
  display: block;
  color: #f2f2f2;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 17px;

}

#navbar a:hover:not(.active) {
  background-color: #ddd;
  color: black;
}

#navbar a.active {
  background-color: #4CAF50;
  color: white;
}

.content {
  padding: 20px 50px 20px 50px;
  font-size: 17;
}

.content2 {
  padding: 20px 50px 20px 50px;
  font-size: 12;
}

.sticky {
  position: sticky;
  top: 0;
  /*width: 100%;*/
  width: 1200px;
  margin: auto;
}

.sticky + .content {
  padding-top: 60px;
}

#wrapper {
  /*border: 1px solid #000;*/
  width: 1200px;
  margin: auto;
}

.container {
  position: relative;
  text-align: center;
  
  
}

.centered {
  position: absolute;
  color: white;
  top: 100px;
  left: 50%;
  transform: translate(-50%, -50%);
  
}

.team_info{
  text-align: center;
  /*width: 150px;*/
  width: 240px;
  /*padding: 50px 50px;*/
}

.team_photo{

  background-color: white;
  border-radius: 50px;
  /*border: 1px solid #ddd;*/
/*  padding-left: 5%;
  padding-right: 5%;*/
  /*width: 80%;*/
  width: 240px;
  height: 420px;
  box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
  margin-bottom: 25px;

}
.team_photo img{
  width: 90%;
  max-height: 80%;
  /*height: auto;*/
  border-radius: 50px;
    margin-top: 5%;
    margin-left: 5%;
    margin-right: 5%;

}

.iconify {
  width: 24px;
  height: 24px;
  padding-top: 10px;
  padding-bottom: 0px;
  padding-left: 3px;
  padding-right: 3px;
}

/*.text{
  font-size: 
}*/
</style>
</head>

  <body style="background-color:white">
    <div class="container" id="home">
      <!---
        <img src="EssAI.png" alt="pen" style="width:1200px;height:500px;">
        <br><br>
      -->
    </div>

    <div id="navbar">
        <a href="#home">Home</a>
        <a href="#about">About</a>
        <a href="#features">Features</a>
        <a href="#demo">Demo</a>
        <a href="#details">Details</a>
        <a href="#team">Team</a>
        <a href="https://github.com/wcex1994/EssAI" target="_blank">GitHub</a>
    </div>

    <div id="wrapper">
      <div style="background-color:white" id="home">
        <div class="content">
          <h1 id="home">Introducing EssAI</h1>
          <p style="text-align: justify; text-justify: inter-word;">
            EssAI is a Google Docs extension that provides exercises for high school and college writing instruction. We've designed two machine-learning enabled exercises that are intended to be:
            <ul style="text-align: justify; text-justify: inter-word;">
              <li><strong>Scalable:</strong> EssAI provides easy-to-use and repeatable writing training that can be customized by instructors across fields and domains.</li>
              <li><strong>Systematic:</strong> EssAI uses machine learning to enable a rigorous form of training called deliberate practice (described below), both by automating away tedious aspects of the exercises and by providing tight feedback loops.</li>
            </ul>
            Our current beta version is framed around the writing styles of Jane Austen and Mark Twain.
            <br><br>
            <img alt="EssAI_home" src="EssAI_home.gif" style="width:1000px;height:500px;">
          </p>
        </div>
      </div>

      <div class="content" id="about" style="background-color:#FADBD8"><br>
          <h1 id="about">About</h1>
          <p style="text-align: justify; text-justify: inter-word;">
            Writing is a vital skill. Unfortunately, it's difficult to apply the best pedagogical practices identified in other fields to writing <sup>[1]</sup>.
            In particular, writing is often not conducive to "deliberate practice"  (DP), which is seen in the skill development literature as a particularly
            efficient path to mastery in domains as diverse as chess, athletics, and even creative fields like music <sup>[2]</sup>. DP is characterized by instant feedback, 
            clear targets, and hyper-focused reflection on the exact nature of errors. All of those are challenging to provide in the writing context: papers can 
            take weeks to grade, the goal of "good writing" is hazy at best, and without a clear sense of their target, students can't possibly reflect on errors 
            at the resolution that DP demands.
            <br><br>
            To help make writing practice more rigorous and more scalable, EssAI uses machine learning to streamline and modernize some classic exercises <sup>[3]</sup> in writing 
            pedagogy that lend themselves to deliberate practice. 
            <br><br>
          </p>
          <p style="text-align: justify; text-justify: inter-word;">
            <em><strong>
            "This tool has the potential to be a more valuable resource to students of creative writing than any given assignment or exercise I've seen currently circulated at the undergraduate level.” 
            <br><br>- Pilot User (Novelist & Writing Instructor)
            </em></strong>
          </p>
          <br>
          <p>
            <sub>[1] Kellogg, R.T., Raulerson, B.A. Improving the writing skills of college students. Psychonomic Bulletin & Review 14, 237–242 (2007). 
              <a href="https://doi.org/10.3758/BF03194058">https://doi.org/10.3758/BF03194058</a></sub><br>
            <sub>[2] Ericsson, K. A., Krampe, R. T., & Tesch-Römer, C. (1993). The role of deliberate practice in the acquisition of expert performance. 
              Psychological Review, 100(3), 363-406. doi:10.1037/0033-295X.100.3.363</sub><br>
            <sub>[3] Edward P. J. Corbett. “The Theory and Practice of Imitation in Classical Rhetoric.” College Composition and Communication, vol. 22, 
              no. 3, 1971, pp. 243–250. JSTOR, www.jstor.org/stable/356450. Accessed 14 Apr. 2021.</sub>
          </p>
      </div>


        <div style="background-color:white" id="features"><br>
            <div class="content">
                <h1 id="features">Features</h1>
                <p style="text-align: justify; text-justify: inter-word;">
                  <ul style="text-align: justify; text-justify: inter-word;">
                    <li>
                      <strong>EssAI <em>Primer</em>:</strong> Practice through Comparison
                      <p>
                        In EssAI Primer, we update a centuries-old writing routine made famous by Ben Franklin, where students rewrite the works of admired authors in their own words,
                        then analyze how more experienced practitioners handled the same content. By fixing content, we clear the noise so that students can focus on style. Our algorithms
                        automate away tedious aspects of the process and provide new ways of analyzing writing structure.
                        <ul>
                          <li><strong>Sentence imitation practice</strong> - User can perform Benjamin Franklin <sup>[1]</sup> style writing practice.</li>
                          <li><strong>Ten stylometric metrics as feedback</strong> - User can compare ten sentence-level stylometry metrics <sup>[2]</sup></li>
                        </ul>
                      </p>
                    </li>
                    <br><br>
                    <li>
                      <strong>EssAI <em>Pro</em>:</strong> Practice through Emulation
                      <p>
                        In EssAI pro, we ask students to replicate the styles of master writers like Jane Austen or Mark Twain. In a kind of reverse Turing Test, they then try to "trick" 
                        our system's text classifier (BERT) into thinking that their text was written by the original author. Students receive instant, continual feedback on which sentences 
                        were convincing and which were not, and hunt for patterns among their misses and hits before rewriting. With this exercise, we've attempted to develop an engaging cycle 
                        of attempt, feedback, reflection, and tuning.
                        <ul>
                          <li><strong>BERT sentence-level text classification</strong> - User can identify which sentence is a good representation of the target author’s style and which to enhance.
                             The BERT model is trained upon a pool of books from Project Gutenberg. <sup>[3]</sup></li>
                        </ul>
                      </p>
                    </li>
                  </ul>
                  <br>
                  <p>
                    <sub>[1] <a href="https://thewritepractice.com/writing-lessons-benjamin-franklin/">https://thewritepractice.com/writing-lessons-benjamin-franklin/</a></sub><br>
                    <sub>[2] We selected the ten stylometry metrics based on user feedback.</sub><br>
                    <sub>[3] <a href="https://www.gutenberg.org/">https://www.gutenberg.org/</a></sub>
                  </p>
                </p>
            </div>
        </div>


        <div class="content" id="demo" style="background-color:#FADBD8"><br>
          <h1 id="demo">Product Demo</h1>   
          <iframe width="1120" height="630" src="https://www.youtube.com/embed/VtXsrRLgjr0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>

        <div class="content" id="details" style="background-color:white"><br>
          <h1 id="details">How It Works</h1>
          <ol>
            <li><strong>Solution Overview</strong></li>
            <p style="text-align: justify; text-justify: inter-word;">
              <strong>EssAI Primer</strong>
              <br><br>
              Users will first select a sample paragraph from the author and EssAI Primer will separate paragraphs to sentence level, 
              offering some text structure hints. Then the user can blur original sentences and try to rewrite the context in their own understanding. 
              Popular stylometric metrics based on user feedback (built upon stylometric analysis from research papers <sub>[1]</sub>) are also calculated for each sentence pair as feedback for comparison. 
              <br><br>
              <img alt="EssAI_Primer_technical" src="EssAI_Primer_technical.png" style="width:1000px;height:500px;">
            </p>
            <br><br>
            <p style="text-align: justify; text-justify: inter-word;">
              <strong>EssAI Pro</strong>
              <br><br>
              Users will first type in paragraphs to be evaluated and select the author to be compared with. 
              EssAI Pro will clean the input texts and split them into sentence-level for BERT evaluation, 
              which will return authorship classification probabilities. Users will see color-coded sentences based on probabilities.
              <br><br>
              <img alt="EssAI_Pro_technical" src="EssAI_Pro_technical.png" style="width:1000px;height:500px;">
            </p>
            <br><br>
            <li><strong>System Architecture</strong></li>
            <br>
            <img alt="EssAI_system_architecture" src="EssAI_system_architecture.png" style="width:1000px;height:500px;">
            <p style="text-align: justify; text-justify: inter-word;">
              <ul style="text-align: justify; text-justify: inter-word;">
                <li>The front-end of EssAI is on Google Doc Extension in JavaScript, CSS and HTML.</li>
                <li>The data manipulation pipelines are deployed in AWS Lambda. Depending on the front-end request, Lambda will either return stylometric calculation or trigger BERT inference. </li>
                <li>Google Colab and Azure Machine Learning Service (AML) are used for model training and tracking.</li>
                <li>The final models are deployed as real-time inference endpoints on Azure Kubernetes Services (AKS) with GPU. Due to cost management, we pause our AKS cluster for most of the time. If you want to have a demo, please contact us.</li>
              </ul>
            </p>
            <br><br>
            <li><strong>Model Architecture - BERT</strong></li>
            <p style="text-align: justify; text-justify: inter-word;">
              Bidirectional Encoder Representations from Transformers (BERT) is published by Google AI in 2017, with application of the 
              bidirectional training of Transformer, a popular attention model, to language modelling <sub>[2]</sub>. By using a masked language model, 
              it is now possible to learn the context of each word from the words appearing both before and after it. 
              <br><br>
              We mostly followed the BertAA: BERT fine-tuning for Authorship Attribution <sub>[5]</sub> implementation. We used the pre-trained BERT model 
              (BERT-base) with 12-layer, 768-hidden, 12-heads, 110M parameters that was trained on English text using 3.3 billion words total [3]. 
              We freeze the entire architecture and attach a dense layer and a softmax layer to the architecture as fine-tuning for classification. 
              One difference we made was that instead of all authors belonging in one BERT model, we separated each writer for one BERT model. 
              The reason is because our users would probably not care how similar they are to other writers who are not their target author, and the 
              BERT model should serve its purpose to identify similarity to the one target author selected. 
              <br><br>
              Additionally, to understand better how the “black box” BERT explains the style classification, we used an open-source library, 
              interpret-text <sub>[4]</sub>, for model interpretability. It offers visualizations on the word-level local feature importance. However, as an 
              overhead, the library requires to use its own BERT wrapper (not compatible with self-implemented Hugging Face model). This restricts 
              some tuning opportunities for accuracy enhancement.
            </p>
            <br><br>
            <li><strong>Model Evaluation</strong></li>
            <p style="text-align: justify; text-justify: inter-word;">
              We use classification accuracy as the evaluation metric. Different authors have very distinctive accuracies. We can reach 99.1% for 
              Jane Austen and 95.4% for Mark Twain. For some other authors that are not included in the final MVP scope, such as Scott Fitzgerald 
              and Charles Dickens, we ended at 60.3% and 72.6%. 
              <br><br>
              To enhance the accuracy, we first tried to expand the datasets. Since we trained one BERT model for each author, each can have its 
              own dataset. Thus, we included more books from random authors into our pool. This method increased the accuracy for Mark Twain. We 
              also tuned several hyper-parameters, such as number of epochs, batch size, and max token length, but they did not give significant 
              enhancement. With the limitation of the BERT wrapper we implemented with the open source library interpret-text, unfortunately, 
              there were only few tuning parameters available.
              <br><br>
              Additionally, we tried to use 29 stylometric metric values as proxies for representing the text style, plug them into a simple 
              logistic regression, and evaluate the classification accuracy. We did not see much benefit out of the initial few experiments of 
              the stylometric logistic regression.
              <br><br>
              Apart from the train-validate-test dataset for famous authors’ books, we also evaluated our BERT models with the blog corpus <sub>[6]</sub> 
              to see how it reacted to daily/common text. We extracted 2000 randomly selected blogs, separated to sentence level, and evaluated 
              their similarity to Jane Austen and Mark Twain. As expected, the majority of sentences were identified as not similar to the target 
              author. For example, only 4000 out of 35000 sentences were marked as Jane Austen style.
            </p>
            <br><br>
            <li><strong>Model Future Development</strong></li>
            <p style="text-align: justify; text-justify: inter-word;">
              Initially we have four authors within our MVP scope: Scott Fitzgerald, Charles Dickens, Jane Austen, and Mark Twain. With concerns 
              of Scott Fitzgerald and Charles Dickens model accuracies and potential misleading results, we decided to restrict our scope to be 
              Jane Austen and Mark Twain for our MVP.
              <br><br>
              However, by running BERT AutoML pipelines for Scott Fitzgerald and Charles Dickens with the existing datasets, we saw models could 
              reach 99.18% and 97.56% respectively. The tuning limitation might come from our BERT wrapper implementation from the interpret-text 
              library. Future development can look at developing Scott Fitzgerald and Charles Dickens models directly from Hugging Face pre-trained models.
            </p>
            <br><br>
            <li><strong>References</strong></li>
              <br>[1] https://github.com/Hassaan-Elahi/Writing-Styles-Classification-Using-Stylometric-Analysis
              <br>[2] https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
              <br>[3] https://arxiv.org/pdf/1706.03762.pdf
              <br>[4] https://github.com/interpretml/interpret-text
              <br>[5] http://publications.idiap.ch/downloads/papers/2020/Fabien_ICON2020_2020.pdf
              <br>[6] https://u.cs.biu.ac.il/~koppel/BlogCorpus.htm        
          </ol>
        </div>

        <div style="background-color:#FADBD8"><br>
            <div class="content2">
                <h1 id="team">About Us</h1>
                <p style="text-align: justify; text-justify: inter-word;">
                  We are a team of UC Berkeley Master of Information and Data Science students.
                  <br>Our mission is to use the latest advances in data science to facilitate general education in a scalable way. 
                </p>
                <br>
                <h1>Team Members</h1>
                <!--<p>-->
                    <table cellpadding="20px">
                        <tr>
                            <td>
                                <div class="team_photo">
                                    <img alt="Kyle Photo" src="kyle.png">
                                    <div class="team_info">
                                        <br><br><p>Kyle Eschen<br />
                                            <a href="mailto:kyleeschen@ischool.berkeley.edu" style="text-decoration:none;">
                                              <span class="iconify" data-icon="carbon:email" data-inline="false"></span>
                                            </a>
                                        </p>
                                    </div>
                                </div>
                            </td>
                            <td>
                                <div class="team_photo">
                                    <img alt="Girija Photo" src="girija.png">
                                    <div class="team_info">
                                      <br><br><p>Girija Ghali<br />
                                            <a href="mailto:girijaghali@ischool.berkeley.edu" style="text-decoration:none;">
                                              <span class="iconify" data-icon="carbon:email" data-inline="false"></span>
                                            </a>
                                            <a href="https://www.linkedin.com/in/girijaghali/" target="_blank" style="text-decoration:none;">
                                              <span class="iconify" data-icon="logos:linkedin-icon" data-inline="false"></span>
                                            </a>
                                        </p>
                                    </div>
                                </div>
                            </td>
                            <td>
                                <div class="team_photo">
                                    <img alt="Hailey Photo" src="hailey.png">
                                    <div class="team_info">
                                      <br><br><p>Hailey Wu<br />
                                            <a href="mailto:haileywu@ischool.berkeley.edu" style="text-decoration:none;">
                                              <span class="iconify" data-icon="carbon:email" data-inline="false"></span>
                                            </a>
                                            <a href="https://www.linkedin.com/in/haileywuqianqian/" target="_blank" style="text-decoration:none;">
                                              <span class="iconify" data-icon="logos:linkedin-icon" data-inline="false"></span>
                                            </a>
                                        </p>
                                    </div>
                                </div>
                            </td>
                            <td>
                                <div class="team_photo">
                                    <img alt="Stanley Photo" src="stanley.png">
                                    <div class="team_info">
                                      <br><br><p>Stanley Ye<br />
                                        <a href="mailto:stanley.ye@berkeley.edu" style="text-decoration:none;">
                                          <span class="iconify" data-icon="carbon:email" data-inline="false"></span>
                                        </a>
                                        <a href="https://www.linkedin.com/in/stanley-ye/" target="_blank" style="text-decoration:none;">
                                          <span class="iconify" data-icon="logos:linkedin-icon" data-inline="false"></span>
                                        </a>
                                        <a href="https://github.com/Stanley-Ye/" target="_blank" style="text-decoration:none;">
                                          <span class="iconify" data-icon="akar-icons:github-fill" data-inline="false"></span>
                                        </a>
                                      </p>
                                    </div>
                                </div>
                            </td>
                        </tr>
                    </table>
                <!--</p>-->
              <br><br><br><br>
            </div>
        </div>
        <script>
        window.onscroll = function() { myFunction() };

        var navbar = document.getElementById("navbar");
        var sticky = navbar.offsetTop;

        function myFunction() {
            if (window.pageYOffset >= sticky) {
                navbar.classList.add("sticky")
            } else {
                navbar.classList.remove("sticky");
            }
        }
        </script>
    </div>
</body>
</html>
