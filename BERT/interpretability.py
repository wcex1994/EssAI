# -*- coding: utf-8 -*-
"""Interpretability.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YkCnQuhEGSanuL8DjpbTWqlgGM8QpFdh
"""

QUICK_RUN = True

"""Ref: https://github.com/interpretml/interpret-text/blob/master/notebooks/text_classification/text_classification_unified_information_explainer.ipynb"""

import torch

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

!pip install interpret_text
import json
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import torch.nn as nn
from matplotlib import pyplot as plt

from interpret_text.experimental.common.utils_bert import Language, Tokenizer, BERTSequenceClassifier
from interpret_text.experimental.common.timer import Timer

from interpret_text.experimental.unified_information import UnifiedInformationExplainer
from google.colab import drive

drive.mount('/content/drive', force_remount=True)

"""## Data"""

dirname = '/content/drive/MyDrive/Capstone/Hailey_data/Jane_128/Pride_and_Prejudice_Jane_Austen_chunkByLength_128.csv'
jane1 = pd.read_csv(dirname)
print("Jane Pride_and_Prejudice: {0} ".format(jane1.shape))

dirname = '/content/drive/MyDrive/Capstone/Hailey_data/Jane_128/Emma_Jane_Austen_chunkByLength_128.csv'
jane2 = pd.read_csv(dirname)
print("Jane Emma: {0}" .format(jane2.shape))

dirname = '/content/drive/MyDrive/Capstone/Hailey_data/Jane_128/The_Great_Gatsby_chunkByLength_128.csv'
Fitzgerald1 = pd.read_csv(dirname)
print("Fitzgerald The_Great_Gatsby: {0}".format(Fitzgerald1.shape))

dirname = '/content/drive/MyDrive/Capstone/Hailey_data/Jane_128/BarnabyRudge_chunkByLength_128.csv'
Dickens1 = pd.read_csv(dirname)
print("Dickens BarnabyRudge: {0}".format(Dickens1.shape))

dirname = '/content/drive/MyDrive/Capstone/Hailey_data/Jane_128/TheAdventuresOfTomSawyer_chunkByLength_128.csv'
Mark1 = pd.read_csv(dirname)
print("Mark TheAdventuresOfTomSawyer: {0}".format(Mark1.shape))

jane1['Label'] = "1"
jane1 = jane1[['Text','Label']]

jane2['Label'] = "1"
jane2 = jane2[['Text','Label']]

Fitzgerald1['Label'] = "0"
Fitzgerald1 = Fitzgerald1[['Text','Label']]

Dickens1['Label'] = "0"
Dickens1 = Dickens1[['Text','Label']]

Mark1['Label'] = "0"
Mark1 = Mark1[['Text','Label']]

df = pd.concat([jane1,jane2,Fitzgerald1,
                     Mark1,Dickens1])

"""## Parameters"""

TRAIN_DATA_FRACTION = 1
TEST_DATA_FRACTION = 1
NUM_EPOCHS = 1

if torch.cuda.is_available():
    BATCH_SIZE = 1
else:
    BATCH_SIZE = 8

DATA_FOLDER = "./temp"
BERT_CACHE_DIR = "./temp"
LANGUAGE = Language.ENGLISH
TO_LOWER = True
MAX_LEN = 280
BATCH_SIZE_PRED = 280
TRAIN_SIZE = 0.7
LABEL_COL = "Label"
TEXT_COL = "Text"

"""### Split"""

# split
df_train, df_test = train_test_split(df, train_size = TRAIN_SIZE, random_state=0)
df_train = df_train.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)

# encode labels
label_encoder = LabelEncoder()
labels_train = label_encoder.fit_transform(df_train[LABEL_COL])
labels_test = label_encoder.transform(df_test[LABEL_COL])

num_labels = len(np.unique(labels_train))

label_encoder.classes_

print("Number of unique labels: {}".format(num_labels))
print("Number of training examples: {}".format(df_train.shape[0]))
print("Number of testing examples: {}".format(df_test.shape[0]))

"""## Tokenization"""

tokenizer = Tokenizer(LANGUAGE, to_lower=TO_LOWER, cache_dir=BERT_CACHE_DIR)

tokens_train = tokenizer.tokenize(list(df_train[TEXT_COL]))
tokens_test = tokenizer.tokenize(list(df_test[TEXT_COL]))

df_train.Text[0]

tokens_train[0]

len(tokens_train[0])

print('Max sentence length: ', max([len(sen) for sen in tokens_train]))

length = []  
for sen in tokens_train:
  length.append(len(sen))

plt.hist(length)

tokens_train, mask_train, train_type_ids = tokenizer.preprocess_classification_tokens(tokens_train, MAX_LEN)
tokens_test, mask_test, test_type_ids = tokenizer.preprocess_classification_tokens(tokens_test, MAX_LEN)

classifier = BERTSequenceClassifier(language=LANGUAGE, num_labels=num_labels, cache_dir=BERT_CACHE_DIR)

"""## Train Model"""

with Timer() as t:
    classifier.fit(token_ids=tokens_train,
                    input_mask=mask_train,
                    labels=labels_train,    
                    num_epochs=NUM_EPOCHS,
                    batch_size=BATCH_SIZE,    
                    verbose=True)    
print("[Training time: {:.3f} hrs]".format(t.interval / 3600))

"""## Score Model"""

preds = classifier.predict(token_ids=tokens_test, 
                           input_mask=mask_test, 
                           batch_size=BATCH_SIZE_PRED)

"""## Checkpoint"""

filename = "/content/drive/MyDrive/Capstone/checkpoints/Jane128.pt"
torch.save({
            'model_state_dict': classifier.model.state_dict()
            }, PATH)
#pickle.dump(classifier, open(filename, 'wb'))

filename = "/content/drive/MyDrive/Capstone/checkpoints/Jane128.pt"
checkpoint = torch.load(filename)
classifier.model.load_state_dict(checkpoint['model_state_dict'])

preds = classifier.predict(token_ids=tokens_test, 
                           input_mask=mask_test, 
                           batch_size=BATCH_SIZE_PRED)

report = classification_report(labels_test, preds, target_names=label_encoder.classes_, output_dict=True) 
accuracy = accuracy_score(labels_test, preds)
print("accuracy: {}".format(accuracy))
print(json.dumps(report, indent=4, sort_keys=True))

"""## Evaluate """

report = classification_report(labels_test, preds, target_names=label_encoder.classes_, output_dict=True) 
accuracy = accuracy_score(labels_test, preds)
print("accuracy: {}".format(accuracy))
print(json.dumps(report, indent=4, sort_keys=True))

"""## Explain"""

device = torch.device("cpu" if not torch.cuda.is_available() else "cuda")

classifier.model.to(device)
for param in classifier.model.parameters():
    param.requires_grad = False
classifier.model.eval()

interpreter_unified = UnifiedInformationExplainer(model=classifier.model, 
                                 train_dataset=list(df_train[TEXT_COL]), 
                                 device=device, 
                                 target_layer=14, 
                                 classes=label_encoder.classes_)

idx = 7
text = df_test[TEXT_COL][idx]
true_label = df_test[LABEL_COL][idx]
predicted_label = label_encoder.inverse_transform([preds[idx]])
print(text, true_label, predicted_label)

predicted_label

text.type

explanation_unified = interpreter_unified.explain_local(text, true_label)

from interpret_text.experimental.widget import ExplanationDashboard

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
ExplanationDashboard(explanation_unified)

sorted_local_importance_names = explanation_unified.get_ranked_local_names()
sorted_local_importance_values = explanation_unified.get_ranked_local_values()

for i in range(99):
    if df_test["Label"][i]:
        print(df_test["Text"][i])
        explanation_unified = interpreter_unified.explain_local(df_test["Text"][i], df_test["Label"][i], label_encoder.inverse_transform([preds[i]]))
        sorted_local_importance_names = explanation_unified.get_ranked_local_names()
        sorted_local_importance_values = explanation_unified.get_ranked_local_values()
        print(sorted_local_importance_names[:4],sorted_local_importance_values[:4])

